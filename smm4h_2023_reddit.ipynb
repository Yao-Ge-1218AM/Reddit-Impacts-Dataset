{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path = '/content/reddit1200'\n",
        "output_file = '/content/reddit-data1000.tsv'\n",
        "\n",
        "with open(output_file, 'wb') as outfile:\n",
        "    for subdir, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(subdir, file)\n",
        "            with open(file_path, 'rb') as infile:\n",
        "                outfile.write(infile.read())"
      ],
      "metadata": {
        "id": "nYL9BtyHIXn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKdK3-lujmu9"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import re;\n",
        "p=re.compile('\\n\\n',re.S);\n",
        "\n",
        "with open(\"/content/reddit-data.tsv\",\"r\") as f:\n",
        "    text = f.read()\n",
        "    #records = text.split(\"#Text=\")\n",
        "    #random_record = random.choice(records)\n",
        "    #print(random_record)\n",
        "\n",
        "    paraList=p.split(text)\n",
        "    fileWriter=open('/content/reddit-1000.tsv','a',encoding='utf8');\n",
        "    for i in range(1000):\n",
        "        random_record = random.choice(paraList)\n",
        "        fileWriter.write(random_record)\n",
        "        fileWriter.write(\"\\n\\n\")\n",
        "    fileWriter.close()\n",
        "    #print(random_record)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import re;\n",
        "p=re.compile('\\n\\n',re.S);\n",
        "\n",
        "with open(\"/content/reddit-data1000.tsv\",\"r\") as f:\n",
        "    text = f.read()\n",
        "    paraList=p.split(text)\n",
        "    random.shuffle(paraList)\n",
        "    fileWriter=open('/content/reddit-1000-random.tsv','a',encoding='utf8');\n",
        "    for x in paraList:\n",
        "        fileWriter.write(x)\n",
        "        fileWriter.write(\"\\n\\n\")\n",
        "    fileWriter.close()\n",
        "    #print(random_record)\n"
      ],
      "metadata": {
        "id": "syb_d2ldP9Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/reddit-1000-random.tsv\",\"r\") as f:\n",
        "  with open(\"/content/reddit-1000-impacts1.tsv\",\"a\") as wf:\n",
        "    txt = f.readlines()\n",
        "    for line in txt:\n",
        "      if(line.startswith(\"#Text=\")):\n",
        "        wf.write(line)\n",
        "        continue\n",
        "      if(line==\"\\n\"):\n",
        "        wf.write(line)\n",
        "        continue\n",
        "      new_line = line.strip(\"\\n\").split(\"\\t\")\n",
        "      #print(line)\n",
        "      if(new_line[3]!=\"_\"):\n",
        "        if(\"Impacts\" in new_line[3] or \"Impacts\" in new_line[4]):\n",
        "          wf.write(line)\n",
        "        else:\n",
        "          wf.write(new_line[0] + \"\\t\" + new_line[1] + \"\\t\" + new_line[2] + \"\\t\" + \"_\" + \"\\t\" + \"_\" + \"\\t\" + \"\\n\")\n",
        "      else:\n",
        "        wf.write(line)\n"
      ],
      "metadata": {
        "id": "JMvHQSN0TTqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/reddit-1000-impacts1.tsv\",\"r\") as f:\n",
        "  with open(\"/content/reddit-1000-impacts2.tsv\",\"a\") as wf:\n",
        "    txt = f.readlines()\n",
        "    for line in txt:\n",
        "      if(line.startswith(\"#Text=\")):\n",
        "        wf.write(line)\n",
        "        continue\n",
        "      if(line==\"\\n\"):\n",
        "        wf.write(line)\n",
        "        continue\n",
        "      new_line = line.strip(\"\\n\").split(\"\\t\")\n",
        "      if(len(new_line)==5):\n",
        "       #print(line)\n",
        "        #wf.write(new_line[0] + \"\\t\" + new_line[1] + \"\\t\" + new_line[2] + \"\\t\" + new_line[3] + \"\\t\" + \"\\n\")\n",
        "\n",
        "        wf.write(new_line[0] + \"\\t\" + new_line[1] + \"\\t\" + new_line[2] + \"\\t\" + \"*\" + \"\\t\" + new_line[3] + \"\\t\" + \"\\n\")\n",
        "      else:\n",
        "        wf.write(line)"
      ],
      "metadata": {
        "id": "11gKgK0CYa_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "with open(\"/content/reddit-1000-impacts2.tsv\",\"r\") as f:\n",
        "  with open(\"/content/reddit-1000-impacts3.tsv\",\"a\") as wf:\n",
        "    txt = f.readlines()\n",
        "    for line in txt:\n",
        "      if(line.startswith(\"#Text=\")):\n",
        "        wf.write(line)\n",
        "        continue\n",
        "      if(line==\"\\n\"):\n",
        "        wf.write(line)\n",
        "        continue\n",
        "      new_line = line.strip(\"\\n\").split(\"\\t\")\n",
        "      if(new_line[3]!=\"_\"):\n",
        "        newstring = re.sub(r'[0-9]+', '', str(new_line[4]))\n",
        "        wf.write(new_line[0] + \"\\t\" + new_line[1] + \"\\t\" + new_line[2] + \"\\t\" + \"*\" + \"\\t\" + newstring.replace(\"[]\",\"\") + \"\\t\" + \"\\n\")\n",
        "      else:\n",
        "        wf.write(line)"
      ],
      "metadata": {
        "id": "9LjuCaS7Z9uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1378 / 843 / 259 / 278"
      ],
      "metadata": {
        "id": "ofB2PFZRlCnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import re;\n",
        "p=re.compile('\\n\\n',re.S);\n",
        "\n",
        "with open(\"/content/reddit-1000-impacts3.tsv\",\"r\") as f:\n",
        "    text = f.read()\n",
        "    #records = text.split(\"#Text=\")\n",
        "    #random_record = random.choice(records)\n",
        "    #print(random_record)\n",
        "\n",
        "    paraList=p.split(text)\n",
        "    fileWriter=open('/content/reddit-impacts-train.tsv','a',encoding='utf8');\n",
        "    for i in range(1378):\n",
        "        random_record = random.choice(paraList)\n",
        "        fileWriter.write(random_record)\n",
        "        fileWriter.write(\"\\n\\n\")\n",
        "    fileWriter.close()\n",
        "    #print(random_record)"
      ],
      "metadata": {
        "id": "bExOhCCKZ9pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "impacts: 384 / 236 / 70 / 78"
      ],
      "metadata": {
        "id": "3DwdeJvWnA1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import re;\n",
        "p=re.compile('\\n\\n',re.S);\n",
        "count = 0\n",
        "\n",
        "with open(\"/content/reddit-impacts-test.tsv\",\"r\") as f:\n",
        "    text = f.read()\n",
        "    #records = text.split(\"#Text=\")\n",
        "    #random_record = random.choice(records)\n",
        "    #print(random_record)\n",
        "\n",
        "    paraList=p.split(text)\n",
        "    for x in paraList:\n",
        "      if(\"Impacts\" in x):\n",
        "        count+=1\n",
        "    print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IR1g760Smp_c",
        "outputId": "38271658-a0c4-414a-f776-2ccf06df0301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import re;\n",
        "p=re.compile('\\n\\n',re.S);\n",
        "count = 0\n",
        "\n",
        "with open(\"/content/reddit-impacts-dev.tsv\",\"r\") as f:\n",
        "    text = f.read()\n",
        "    #records = text.split(\"#Text=\")\n",
        "    #random_record = random.choice(records)\n",
        "    #print(random_record)\n",
        "\n",
        "    paraList=p.split(text)\n",
        "    print(len(paraList))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzWiyfSfnREV",
        "outputId": "80c30422-07b9-49ea-f026-f91c859ab99c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2024-1-6:** 需要把train,dev,test改成BIO格式"
      ],
      "metadata": {
        "id": "bF67aShXgij-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/reddit-impacts-dev.tsv\",\"r\") as f:\n",
        "  with open(\"/content/reddit-impacts-test.txt\",\"w\") as wf:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      if(line!=\"\\n\"):\n",
        "        if(line.startswith(\"#Text=\")==0):\n",
        "          new_line = line.strip(\"\\n\").split(\"\\t\")\n",
        "          if(new_line[4]==\"_\"):\n",
        "            wf.write(new_line[2] + \" \" + \"O\" + \"\\n\")\n",
        "          elif(new_line[4]==\"Abuse/Misuse Clinical Impacts\"):\n",
        "            wf.write(new_line[2] + \" \" + \"I-\" + \"Clinical_Impacts\" + \"\\n\")\n",
        "          elif(new_line[4]==\"Clinical Impacts\"):\n",
        "            wf.write(new_line[2] + \" \" + \"I-\" + \"Clinical_Impacts\" + \"\\n\")\n",
        "          elif(new_line[4]==\"Social Impacts\"):\n",
        "            wf.write(new_line[2] + \" \" + \"I-\" + \"Social_Impacts\" + \"\\n\")\n",
        "          elif(new_line[4]==\"Abuse/Misuse Social Impacts\"):\n",
        "            wf.write(new_line[2] + \" \" + \"I-\" + \"Social_Impacts\" + \"\\n\")\n",
        "      else:\n",
        "        wf.write(line)"
      ],
      "metadata": {
        "id": "oIXswva8gpad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check BIO 给的对不对"
      ],
      "metadata": {
        "id": "6FmSJFW_u-U7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/reddit-impacts-test.txt\",\"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      if(line!=\"\\n\"):\n",
        "        new_line = line.strip(\"\\n\").split(\" \")\n",
        "        if(new_line[1]!=\"O\" and new_line[1]!=\"I-Social_Impacts\" and new_line[1]!=\"I-Clinical_Impacts\"):\n",
        "          print(line)"
      ],
      "metadata": {
        "id": "ufjSYvTLvBHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform data from txt to json"
      ],
      "metadata": {
        "id": "Kf30FMOa5WPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/test.txt\",\"r\") as f:\n",
        "  with open(\"/content/test.json\",\"a\") as wf:\n",
        "    text = f.readlines()\n",
        "    word = []\n",
        "    label = []\n",
        "    for line in text:\n",
        "      if(line==\"\\n\"):\n",
        "        wf.write('{\"text\": [')\n",
        "        count = 0\n",
        "        for w in word:\n",
        "          count+=1\n",
        "          if(count!=len(word)):\n",
        "            wf.write('\"'+w+'\", ')\n",
        "          else:\n",
        "            wf.write('\"'+w+'\"')\n",
        "        wf.write('], \"label\": [')\n",
        "        count = 0\n",
        "        for l in label:\n",
        "          count+=1\n",
        "          if(count!=len(label)):\n",
        "            wf.write('\"'+l+'\", ')\n",
        "          else:\n",
        "            wf.write('\"'+l+'\"')\n",
        "        wf.write(']}' +\"\\n\")\n",
        "        word = []\n",
        "        label = []\n",
        "      else:\n",
        "        new_line = line.strip(\"\\n\").split(\" \")\n",
        "        word.append(new_line[0])\n",
        "        label.append(new_line[1])"
      ],
      "metadata": {
        "id": "gZ3eFbQq2wJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculate the frequency of text"
      ],
      "metadata": {
        "id": "2kKqQGbv5Te-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/train.txt\") as f:\n",
        "  text = f.readlines()\n",
        "  Location = []\n",
        "  mmHg = []\n",
        "  Race = []\n",
        "  Sex = []\n",
        "  Method = []\n",
        "  for line in text:\n",
        "    if(line!=\"\\n\"):\n",
        "      new_line = line.strip(\"\\n\").split(\" \")\n",
        "      if(new_line[1]!=\"O\"):\n",
        "        #print(line)\n",
        "        #SpecificDisease, Modifier, DiseaseClass\n",
        "        if(new_line[1].endswith(\"Location\")):\n",
        "          Location.append(new_line[0])\n",
        "        elif(new_line[1].endswith(\"mmHg\")):\n",
        "          mmHg.append(new_line[0])\n",
        "        elif(new_line[1].endswith(\"Race\")):\n",
        "          Race.append(new_line[0])\n",
        "        elif(new_line[1].endswith(\"Sex\")):\n",
        "          Sex.append(new_line[0])\n",
        "        elif(new_line[1].endswith(\"Method\")):\n",
        "          Method.append(new_line[0])"
      ],
      "metadata": {
        "id": "EjgQsFZ75Rw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Clinical)\n",
        "print(Social)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLIeYkTs6Uwg",
        "outputId": "29b17e05-6706-406d-ae46-ded4cf1502f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['withdrew', 'codeine', 'addict', 'rehab', 'at', 'a', '28', 'day', 'detox', '/', 'rehab', 'overdoses', 'and', 'dies', 'psych', 'ward', 'drug-induced', 'psychosis', 'withdrawal', 'Outpatient', 'Withdrawal', 'addiction', 'WD', 'precipitated', 'withdrawal', 'anxiety', 'restlessness', 'tweaking', 'I', 'tried', 'nine', 'rehabs', 'withdrawals', 'I', 'had', 'a', 'tooth', 'suddenly', 'feel', 'like', 'it', 'was', 'hollow', 'and', 'I', 'saw', 'a', 'big', 'hole', 'in', 'it', 'Its', 'very', 'interesting', 'how', 'u', 'went', 'to', 'AA', 'for', 'so', 'many', 'years', 'bc', 'I', 'did', 'the', 'same', 'withdrawing', 'withdrawal', 'addiction', 'So', 'i', 'did', '45', 'days', 'in', 'a', 'rehab', 'withdrawal', 'teeth', 'pulled', 'Gives', 'you', 'energy', 'and', 'kills', 'pain', 'and', 'anxiety', '9', 'week', 'program', 'addiction', 'I', 'threw', 'myself', 'into', 'pwd', '3', 'fucking', 'times', 'last', 'month', '.', 'addiction', 'had', 'to', 'stop', 'every', 'few', 'minutes', 'to', 'vomit', 'burning', 'acid', '.', 'opiate', 'addiction', 'rehab', 'six', 'months', 'to', 'totally', 'kick', 'it', 'stomach', 'issues', 'codeine', 'addict', 'cotton', 'fever', 'so', 'bad', 'a', 'couple', 'times', 'where', 'I', 'was', 'shaking', 'so', 'bad', 'I', 'was', 'trembling', 'head', 'to', 'toe', 'and', 'thought', 'I', 'was', 'going', 'to', 'die', 'I', 'get', 'out', 'of', 'rehab', 'on', 'march', '6th', 'sleep', 'issues', 'Im', 'doing', 'outpatient', 'stuff', 'now', '.', 'heroin', 'addiction', 'withdrawals', 'addiction', 'rehab', 'blacked', 'out', 'Headaches', 'withdrawal', 'withdrawal', 'I', 'just', 'called', 'a', '1800', 'number', 'and', 'was', 'in', 'a', 'rehab', 'the', 'next', 'day', 'withdrawal', 'WD', 'withdrawal', 'I', 'did', 'a', '28', 'day', 'detox', '/', 'rehab', 'back', 'in', '2018', ',', 'it', 'was', 'a', '12step', 'based', 'rehab', 'nose', 'is', 'killing', 'me', 'I', 'went', 'to', 'nine', 'rehabs', 'Zero', 'motivation', ',', 'feelings', 'of', 'despair', ',', 'panic', ',', 'like', 'I', 'was', 'walking', 'under', 'water', ',', 'etc', '.', 'hurl', 'up', 'a', 'geyser', 'of', 'stomach', 'acid', 'within', 'an', 'hour', 'of', 'dosing', 'Panic', 'attack', 'feeling', '.', 'drug', 'induced', 'psychosis', '(', 'was', '51', '-', '50', '’', 'd', 'for', '3', 'days', 'addiction', 'I', 'have', 'just', 'gone', 'to', 'my', 'drug', 'clinic', 'and', 'low', 'and', 'behold', 'a', 'old', 'school', 'mate', 'from', '15', 'years', 'ago', 'now', 'works', 'there', 'she', 'reads', 'my', 'whole', 'file', ',', 'shocked', 'I', 'was', 'hooked', 'on', 'heroin', ',', 'benzos', ',', 'been', 'to', 'hospital', 'for', 'OD', 'etc', '.', '.', 'I', 'am', 'a', 'recovering', 'addict', 'and', 'I', 'overdosed', 'at', '19', '(', 'I', '’', 'm', '25', 'now', ')', 'Addiction', 'cotton', 'mouth', 'addiction', 'But', 'i', 'felt', 'chills', 'while', 'i', 'was', 'sweating', 'my', 'balls', 'off', 'I', 'started', 'to', 'feel', 'depressed', '.', 'took', 'me', 'over', '6', 'months', 'to', 'even', 'start', 'to', 'become', 'normal', 'detox', 'ready', 'to', 'kill', 'myself', 'This', 'reminds', 'me', 'so', 'much', 'of', 'how', 'I', 'felt', 'my', 'first', 'week', 'in', 'rehab', '.', 'hypochlorhydria', 'lost', 'a', 'pregnancy', 'overdosed', 'It', 'was', 'effed', 'up', 'and', 'then', 'about', 'a', 'week', 'later', 'it', 'fell', 'out', '.', 'ulcers', 'Thats', 'how', 'I', 'think', 'about', 'my', 'own', 'recovery', ',', 'but', 'I', 'know', 'it', \"wouldn't\", 'have', 'happened', 'without', 'going', 'to', 'AA', 'son', 'was', 'born', 'on', 'methadone', 'withdrawal', 'in', 'The', 'NICU', 'addicted', 'PAWS', 'Pain', 'tiredness', 'depression', 'anxiety', 'sleep', 'sweats', 'and', 'chills', 'The', 'tooth', 'that', 'fell', 'out', 'was', 'already', 'fucked', 'up', 'so', 'it', \"wasn't\", 'a', 'big', 'deal', 'but', 'the', 'fact', 'that', 'a', 'second', 'tooth', 'now', 'seems', 'to', 'have', 'a', 'hole', 'in', 'it', 'is', 'making', 'me', 'freak', 'out', 'and', 'think', 'it', 'is', 'an', 'emergency', 'ya', 'anxious', 'To', 'add', ',', 'I', 'too', 'have', 'likely', 'damaged', 'my', 'brain', 'when', 'I', 'was', 'young', '.', 'I', 'was', 'on', 'subs', '10', '+', 'years', 'then', 'got', 'off', 'of', 'them', 'in', 'a', 'rehab', 'for', '10', '-', '11', 'months', 'in', 'which', 'I', 'tried', 'Vivitrol', 'in', 'that', 'time', '.', 'withdrew', 'son', 'was', 'born', 'addicted', 'to', 'methadone', ',', 'sores', 'all', 'over', 'my', 'body', 'addiction', 'I', 'use', 'to', 'feel', 'the', 'same', 'way', 'about', 'groups', 'and', 'therapy', 'until', 'I', 'found', 'the', 'right', 'combo', 'to', 'work', 'for', 'me', 'I', 'got', 'so', 'sick', 'and', 'tired', 'of', 'the', 'stupid', 'AA', 'meetings', 'and', 'NA', 'meetings', 'that', 'just', 'did', 'not', 'work', '.', 'opiate', 'addiction', 'Ive', 'ODed', 'a', 'few', 'times', 'n', 'got', 'brought', 'back', 'But', 'now', 'I', 'test', 'it', 'with', 'doing', 'half', 'of', 'one', 'first', 'because', \"there's\", 'some', 'fire', 'flying', 'around', \"that's\", 'been', 'putting', 'me', 'and', 'my', 'friends', 'in', 'the', 'hospital', '.', 'For', 'myself', ',', 'I', 'went', 'to', 'meetings', 'fresh', 'out', 'of', 'rehab', 'and', 'very', 'vulnerable', '.', 'withdrawal', 'hospital', 'Permanent', 'neurological', 'damage', 'Now', 'about', 'another', 'week', 'later', 'I', 'got', 'another', 'tooth', 'that', 'now', 'has', 'a', 'hole', 'and', 'feels', 'hollow', '.', 'hair', 'loss', 'brain', 'zaps', 'addiction', 'withdrawal', '30', '-', 'day', 'outpatient', 'detox', 'program', 'tweaking', 'Felt', 'hopeless', 'but', 'rehab', 'definitely', 'saved', 'my', 'life', '.', 'i', 'stayed', 'at', 'fucked', 'up', 'rehabs', 'before', 'I', 'did', 'a', '28', 'day', 'detox', '/', 'rehab', 'to', 'get', 'off', 'of', '8mg', 'killed', 'my', 'stomach', 'partial', 'hospitalization', 'program', 'for', 'treatment', 'withdrawing', 'When', 'i', 'was', 'in', 'rehab', 'withdrawal', 'withdrawal', 'pwd', 'tweaking', 'eight', 'day', 'detox', 'I', 'went', 'to', 'a', 'methodone', 'clinic', 'for', 'awhile', '.', 'withdrawing', 'addiction', 'ulcers', 'ulceration', 'of', 'the', 'small', 'intestine', '3', 'rehab', 'od', 'I', 'almost', 'died', 'PWD', 'I', 'am', 'going', 'on', 'my', '3rd', 'therapist', 'from', 'the', 'same', 'place', 'over', '2', 'years', 'suicidal', 'Then', 'I', 'got', 'tired', 'n', 'fell', 'asleep', '.', 'addicted', 'A', 'few', 'weeks', 'later', ',', 'I', 'went', 'to', 'my', 'Suboxone', 'appointment', 'and', 'I', 'did', 'my', 'drug', 'test', 'and', 'it', 'was', 'positive', 'for', 'opioids', 'found', 'myself', 'in', 'the', 'cycle', 'of', 'taking', 'more', 'and', 'more', ',', 'having', 'to', 'rush', 'to', 'buy', 'it', 'and', 'running', 'out', 'before', 'I', 'should', '’', 've', '(', 'basically', ',', 'addiction', ')', '.', 'craving', 'to', 'use', 'psychotic', 'break', ',', 'landing', 'me', 'in', 'the', 'hospital', 'getting', 'extremely', 'depressed', 'opiate', 'addiction', 'that', 'lasted', 'nearly', '18', 'years', 'ended', 'up', 'being', 'committed', 'codeine', 'addiction', 'Before', 'that', 'I', 'had', 'tried', '9', 'rehabs', ',', 'those', 'stupid', 'NA', '12', 'step', 'groups', 'addict', 'overdosed', 'five', 'times', 'BMI', 'was', 'in', 'the', '16', 'range', 'drug', 'induced', 'psychosis', 'ready', 'to', 'kill', 'myself', 'depressed', 'addict', 'addict', 'WD', \"OD'd\", 'drug-induced', 'psychotic', 'break', 'I', 'am', '2', 'years', 'off', 'of', 'benzos', 'and', 'working', 'a', 'sub', 'program', 'I', 'go', 'to', 'AA', 'but', 'NEED', 'more', 'went', 'into', 'treatment', 'Makes', 'the', 'pain', 'tolerable', '.', 'addicted', 'Last', 'time', 'i', 'overdosed', 'in', 'my', 'running', 'car', ',', 'whoever', 'called', 'the', 'cops', 'stole', 'my', 'wallet', 'out', 'of', 'my', 'glove', 'box', 'with', '$2,400', 'American', 'dollars', 'in', 'it', 'opiate', 'addict', 'Chills', 'washed', 'off', 'I', 'perked', 'up', 'my', 'testosterone', 'being', 'slapped', 'away', '2', 'years', 'in', 'rehab', 'and', 'have', 'been', 'off', 'of', 'them', 'since', ',', 'which', 'has', 'been', 'a', 'real', 'struggle', 'not', 'going', 'to', 'lie', 'addicted', 'I', 'started', 'to', 'feel', 'a', 'major', 'depressive', 'wave', 'coming', 'on', '.', 'withdrawal', 'addictions', 'withdrawals', 'about', 'killed', 'me', 'WD', 'addicted', 'I', 'am', 'a', 'recovering', 'addict', 'and', 'I', 'overdosed', 'at', '19', '(', 'I', \"'\", 'm', '25', 'now', ')', 'Meetings', 'and', 'seeing', 'a', 'suboxone', 'doctor', 'Outpatient', \"can't\", 'sleep', 'addiction', 'decade', 'long', 'heroin', 'addiction', 'sick', 'for', 'about', '6', 'months', 'Cop', 'came', 'and', 'I', 'had', 'to', 'be', 'Narcaned', '.']\n",
            "['had', 'no', 'money', ',', 'I', 'hope', 'you', 'choose', 'to', 'stay', 'clean', 'because', 'it', 'is', 'absolutely', 'NOT', 'worth', 'going', 'back', 'to', 'that', 'hell', 'that', 'I', 'just', 'pulled', 'myself', 'out', 'of', 'after', '20', 'years', 'of', 'being', 'a', 'worthless', 'scumbag', 'I', 'was', 'a', 'homeless', 'junkie', 'on', 'the', 'streets', 'We', 'lost', 'everything', 'during', 'our', 'drug', 'use', 'before', 'we', 'met', 'each', 'other', 'and', 'after', '.', 'I', 'was', 'so', 'brainwashed', 'by', 'that', 'hell', 'hole', '.', \"I'm\", 'very', 'thankful', 'that', 'after', 'all', 'of', 'the', 'ods', \"I've\", 'had', 'I', 'never', 'hurt', 'myself', 'with', 'lasting', 'damage', 'that', 'I', 'can', 'tell', 'living', 'in', 'an', 'alley', 'for', '2', 'years', 'I', 'was', 'withdrawing', 'super', 'hard', 'infront', 'of', 'my', 'whole', 'extended', 'family', 'I', 'seriously', 'lost', 'everything', 'that', 'I', 'cared', 'about', 'other', 'then', 'my', 'family', '.', 'I', 'only', 'came', 'here', 'with', 'a', 'couple', 'bags', 'of', 'clothes', 'and', 'my', 'xbox', ',', 'I', 'am', 'truly', 'starting', 'over', 'at', 'this', 'point', 'because', 'the', 'way', 'I', 'was', 'living', 'my', 'life', 'before', ',', 'it', \"'\", 's', 'a', 'miracle', 'that', 'I', 'wasn', \"'\", 't', 'killed', 'by', 'someone', 'or', 'overdosed', 'and', 'died', 'I', 'was', 'worried', \"I'd\", 'lose', 'my', 'job', 'because', \"I'd\", 'be', 'unable', 'to', 'function', 'for', 'a', 'wee', 'and', 'i', 'could', 'not', 'deny', 'any', 'more', 'that', 'using', 'I', 'was', 'no', 'good', 'for', 'anybody', 'and', 'starting', 'to', 'be', 'miserable', 'Ended', 'up', 'getting', 'pulled', 'over', 'with', 'no', 'license', 'and', 'lost', 'my', 'truck', 'and', 'went', 'to', 'jail', 'I', 'was', 'charged', 'with', 'disorderly', 'conduct', '(', 'only', 'thing', 'on', 'my', 'record', ')', 'needs', 'to', 'stop', '!', 'legally', 'cut', 'us', 'off', 'like', 'this', 'from', 'seeing', 'our', 'daughter', '?', 'weird', 'effects', 'with', 'withdrawls', 'I', 'was', 'homeless', 'because', 'of', 'drugs', 'spend', 'all', 'my', 'hard', 'earned', 'pay', 'check', 'on', 'dope', 'homeless', 'A', 'long', 'time', 'ago', 'I', 'once', 'had', 'blacked', 'out', 'for', 'basically', 'a', 'couple', '/', 'few', 'days', 'I', 'started', 'using', 'and', 'I', 'changed', 'and', 'became', 'a', 'different', 'person', 'probation', 'she', 'hadn', \"'\", 't', 'been', 'home', 'for', 'days', 'lost', 'friends', ',', 'family', 'members', 'and', 'a', 'lot', 'of', \"people's\", 'trust', 'I', 'need', 'to', 'do', 'this', 'before', 'I', 'join', 'so', 'many', 'that', 'were', 'my', 'friends', 'that', 'are', 'now', 'dead', '.', 'id', 'been', 'in', 'jail', 'for', 'like', '3.5', 'months', 'I', 'had', 'to', 'wait', 'like', '38', 'hours', 'and', 'it', 'finally', 'worked', '.', 'When', 'I', 'say', 'super', 'humanly', 'sociable', 'I', 'mean', 'I', \"'\", 'm', 'a', 'fucking', 'beast', 'at', 'basically', 'everything', 'I', 'do', 'when', 'I', \"'\", 'm', 'on', 'dissos', \"I'm\", 'pretty', 'sure', 'I', 'ruined', 'my', 'life', 'though', 'in', 'all', 'seriousness', 'But', 'when', 'I', 'got', 'into', 'heroin', ',', 'rent', \"couldn't\", 'be', 'bothered', 'with', 'anymore', 'I', 'could', 'work', 'but', 'and', 'pay', 'bills', 'but', 'I', 'was', 'paying', 'bills', 'late', 'and', 'constantly', 'making', 'excuses', 'at', 'work', '.', 'I', 'have', 'definitely', 'been', 'to', 'rock', 'bottom', 'at', 'points', 'in', 'my', 'life', 'sad', 'Til', 'I', 'had', 'blown', 'my', 'very', 'last', 'dollar', 'losing', 'car', ',', 'home', ',', 'and', 'possessions', '.', 'I', 'was', 'homeless', 'like', '4', 'months', 'ago', '.', 'And', 'I', 'never', 'thought', 'I', 'would', 'live', 'to', 'see', '40', ',', 'but', 'here', 'I', 'am', '39', 'and', 'going', 'strong', '.', 'I', 'lost', 'my', 'best', 'friend', 'I', 'lost', 'my', 'job', 'homeless', 'I', 'felt', 'terrible', '.', 'homeless', 'restlessness', 'My', 'girlfriend', 'of', '1.5yrs', 'left', 'me', 'the', 'day', 'after', 'she', 'felt', 'we', 'had', 'no', 'future', 'together', 'I', 'just', 'want', 'to', 'stop', 'feeling', 'like', \"I'm\", 'dying', 'inside', \"I've\", 'dealt', 'with', 'the', 'same', 'thing', ',', 'being', 'on', 'probation', 'and', 'not', 'being', 'able', 'to', 'smoke', '.', 'She', 'would', 'disappear', 'for', 'a', 'week', 'at', 'a', 'time', 'when', 'I', 'was', 'a', 'kid', '.', 'talkative', 'deep', 'debt', 'I', 'would', 'have', 'been', 'dead', 'or', 'just', 'another', 'shit', 'stain', 'on', 'society', '.', 'homeless', 'My', 'partner', 'killed', 'himself', ',', 'I', 'lost', 'my', 'kid', 'for', 'awhile', ',', 'my', 'job', ',', 'and', 'every', 'possession', 'and', 'piece', 'of', 'clothing', 'I', 'owned', 'I', \"couldn't\", 'have', 'a', 'normal', 'life', 'because', 'you', \"can't\", 'use', 'everyday', 'and', 'hide', 'it', 'for', 'long', 'in', 'a', 'serious', 'relationship', '.', 'but', 'the', 'shame', ',', 'the', 'isolation', 'and', 'the', 'heavy', 'secrets', 'were', '.', 'mom', 'was', 'still', 'running', 'the', 'streets', 'and', 'ending', 'up', 'in', 'the', 'hospital', 'every', 'so', 'often', '.', 'They', \"'\", 're', 'mad', 'at', 'me', 'because', 'Andrew', 'OD', \"'\", 'd', 'while', 'I', 'was', 'with', 'him', '.', 'arrested', 'and', 'charged', 'with', 'a', 'felony', 'I', 'was', 'charged', 'with', 'disorderly', 'conduct', '(', 'only', 'thing', 'on', 'my', 'record', ')', 'He', 'lost', 'a', '80k', '/', 'yearly', 'job', 'and', 'I', 'lost', 'my', 'family', 'probation', 'but', 'the', 'shame', ',', 'the', 'isolation', 'and', 'the', 'heavy', 'secrets', 'were', '.', 'As', 'so', 'many', 'have', 'said', 'now', 'I', 'have', 'to', 'deal', 'with', 'all', 'the', 'problems', 'I', 'put', 'off', 'getting', 'high', ',', 'so', 'I', 'hear', 'u', '.', 'Took', '3', 'days', 'to', 'feel', 'okay', '.', 'age', '40']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "punctuation_string = string.punctuation\n",
        "#print(punctuation_string)\n",
        "\n",
        "Method_new = []\n",
        "\n",
        "for word in Method:\n",
        "  for i in punctuation_string:\n",
        "    word = word.replace(i,\"\")\n",
        "  #print(word)\n",
        "  Method_new.append(word.lower())"
      ],
      "metadata": {
        "id": "QPG2IMIQ6bde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Method)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H8pyfXN6fhN",
        "outputId": "37f8eeca-8f12-4995-9e8c-777ad88fe6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['intensive', 'intensive', 'Oscillometric', 'oscillometric', 'office', 'methods', 'office', 'BP', 'measurements', 'noninvasive', 'reduction', 'intensive', 'intensive', 'intensive', 'intensive', 'oscillometric', 'cuffless', 'mercury', 'sphygmomanometers', 'oscillometric', 'devices', 'mercury', 'sphygmomanometer', 'sphygmomanometers', 'mercury', 'sphygmomanometers', 'mercury', 'sphygmomanometers']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "frequency = collections.Counter(Method_new)\n",
        "print(frequency)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dBk1Wgu6jIV",
        "outputId": "c1533115-34d4-47f5-c01c-f18c14204326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'intensive': 6, 'oscillometric': 4, 'mercury': 4, 'sphygmomanometers': 4, 'office': 2, 'methods': 1, 'bp': 1, 'measurements': 1, 'noninvasive': 1, 'reduction': 1, 'cuffless': 1, 'devices': 1, 'sphygmomanometer': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IO to BIO"
      ],
      "metadata": {
        "id": "eTA6FY5cTFU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/reddit-impacts-test-BIO.txt\",\"r\") as f:\n",
        "  with open(\"/content/reddit-impacts-test-BIO-new.txt\", \"w\") as wf:\n",
        "    lines = f.readlines()\n",
        "    i = 1\n",
        "    while(i<len(lines)):\n",
        "      line = lines[i]\n",
        "      if(line!=\"\\n\"):\n",
        "        new_line = line.strip(\"\\n\").split(\" \")\n",
        "        if(lines[i-1]!=\"\\n\"):\n",
        "          pre_line = lines[i-1].strip(\"\\n\").split(\" \")\n",
        "          #print(new_line)\n",
        "          if(new_line[1].startswith(\"I-\") and pre_line[1].startswith(\"I-\")==0 and pre_line[1].startswith(\"B-\")==0):\n",
        "            line = line.replace(\"I-\",\"B-\")\n",
        "            #print(line)\n",
        "            wf.write(line)\n",
        "          else:\n",
        "            wf.write(line)\n",
        "        else:\n",
        "          if(new_line[1].startswith(\"I-\")):\n",
        "            line = line.replace(\"I-\",\"B-\")\n",
        "            wf.write(line)\n",
        "          else:\n",
        "            wf.write(line)\n",
        "      else:\n",
        "        wf.write(line)\n",
        "      i = i+1"
      ],
      "metadata": {
        "id": "D1gTmrJWTHiA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}